{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Scientist Opening Pre-employment Assessment: Second Exercise</h1><br>\n",
    "<p>Ben Trey</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bentrey/myProjectDir/myProjectEnv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm_notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 0:</h1>\n",
    "<p>A few words of caution:<br> \n",
    "1) Read all the way through the instructions.<br> \n",
    "2) Models must be deployed as an API using Python.<br>\n",
    "3) No additional data may be added or used.<br> \n",
    "4) Not all data must be used to build an adequate model, but making use of complex variables will help us identify high-performance candidates.<br>\n",
    "5) The predictions returned by the API should be the class probabilities for belonging to the positive class, not the class itself (i.e. a decimal value, not just 1 or 0).</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Given Model</b><br>\n",
    "  Here is a brief summary of the operation of the given model. It will not     work on this dataset so I am going to modify it to analyze its     \n",
    "  performance. I am going to try to keep as much orginal code as   \n",
    "  possible.<br>\n",
    "  <ul><li>Loads Data<br>\n",
    "      Drops nan\n",
    "      Combines train and test data\n",
    "      Gets percentage of Null values for each feature\n",
    "    </li>\n",
    "    <li>Feature Selection<br>\n",
    "        Drops features not used for modeling<br>\n",
    "        Drops features requiring Natural Language Processing<br>\n",
    "        Removes Redundant features\n",
    "    </li>\n",
    "    <li>Feature Engineering<br>\n",
    "      Change dollar figures represented as strings to floats<br>\n",
    "      Turns percentages represented as strings to floats<br>\n",
    "      Creates new features to be used in modeling\n",
    "      Converts date times from strings into float\n",
    "      Creates a new feature\n",
    "      Sets target variable\n",
    "    </li>\n",
    "    <li>Split input variables into numerical and categorical features\n",
    "    </li>\n",
    "    <li>Data Preperation for Tree Model<br>\n",
    "      Replace null value with large number<br>\n",
    "      Label encoding for categorical feature\n",
    "    </li>\n",
    "    <li>Split Into Training and Test Dataset<br>\n",
    "    </li>\n",
    "    <li>Build Random Forest Model\n",
    "    </li>\n",
    "    <li>Develop A Random Forest Model\n",
    "    </li>\n",
    "    <li>Save the Model in Local Disk\n",
    "    </li>\n",
    "    <li>Evaluate the AUC Performance of the Model\n",
    "    </li>\n",
    "    <li>Make Prediciton on Test Data\n",
    "    </li>\n",
    "  </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Replicating Given Model</b><br>\n",
    "The code below is a faithful represenation on the given model. Some of the libraries were changed in order to run with the current version of sklearn. The data in the given model did not match the column names of the given data. The feature engineering in the given model was applied appropriately to the given data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    raw_train=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_train.csv')\n",
    "    raw_test=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_test.csv')\n",
    "    raw_train_drop = raw_train.dropna(axis=0, subset=['y'])\n",
    "    list_all=[raw_train_drop,raw_test]\n",
    "    len_train = len(raw_train_drop)\n",
    "    len_test =len(raw_test)\n",
    "    raw=pd.concat(list_all, ignore_index=True, sort=False)\n",
    "    del(raw_train,raw_test,raw_train_drop)\n",
    "    return [raw, len_train, len_test]\n",
    "\n",
    "def get_null(raw):\n",
    "    Null_list = raw.isnull().sum().sort_values(ascending=False)/float(raw.shape[0])*100\n",
    "    print('the pencentage of NUll value in each features are:\\r\\n', Null_list[:10])\n",
    "\n",
    "def remove_features(features, raw):\n",
    "    for feature in features:\n",
    "        del raw[feature]\n",
    "    return raw\n",
    "\n",
    "def remove_dollar(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('$').replace(',',''))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_dollars(features, raw):\n",
    "    for feature in features:\n",
    "        raw[feature]=raw[feature].apply(remove_dollar)\n",
    "    return raw\n",
    "\n",
    "def per_float(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('%'))/100\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_percents(features, raw):\n",
    "    for feature in features:\n",
    "        raw[feature]=raw[feature].apply(per_float)\n",
    "    return raw\n",
    "\n",
    "def set_target_varible(raw):\n",
    "    return raw['y']\n",
    "\n",
    "def set_input_variable_list(raw):\n",
    "    del raw['y']\n",
    "    return raw\n",
    "\n",
    "def get_object_columns(raw):\n",
    "    return raw.dtypes[raw.dtypes == 'object'].index\n",
    "\n",
    "def get_numerical_columns(raw):\n",
    "    return raw.dtypes[raw.dtypes == 'float64'].index\n",
    "\n",
    "def fill_null(raw,value=10**20):\n",
    "    labels=get_numerical_columns(raw)\n",
    "    for label in labels:\n",
    "        raw[label].fillna(value,inplace=True)\n",
    "    return raw    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pencentage of NUll value in each features are:\r\n",
      " y      20.000\n",
      "x13     0.034\n",
      "x55     0.034\n",
      "x42     0.034\n",
      "x18     0.032\n",
      "x62     0.030\n",
      "x99     0.030\n",
      "x24     0.030\n",
      "x96     0.028\n",
      "x63     0.028\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "raw = raw_data[0]\n",
    "len_train = raw_data[1]\n",
    "len_test = raw_data[2]\n",
    "get_null(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = convert_dollars(['x12'], raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = convert_percents(['x79'], raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y = raw['y']\n",
    "del raw['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = get_object_columns(raw)\n",
    "num_cols = get_numerical_columns(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = fillNull(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label encoding for categorical feature\n",
    "from sklearn import preprocessing\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "dict_list = []\n",
    "for i in cat_cols:\n",
    "    raw[i] = LBL.fit_transform(raw[i].fillna('0'))\n",
    "    j = dict(zip(np.arange(len(LBL.classes_)),LBL.classes_))\n",
    "    k = {i:j}\n",
    "    dict_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = raw[:len_train]\n",
    "y = raw_y[:len_train]\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "holdout_x = raw[len_train:]\n",
    "holdout_y = raw_y[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start the hypermeter grid search for n_estimator, it may take a few minutes\n",
      " hypermeter grid search is over\n",
      "The best paramter for n_estimator is: 180\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [60,120,180]\n",
    "}\n",
    "\n",
    "print('start the hypermeter grid search for n_estimator, it may take a few minutes')\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(train_x, train_y)\n",
    "\n",
    "best_estimators = list(CV_rfr.best_params_.values())[0]\n",
    "\n",
    "print(' hypermeter grid search is over')\n",
    "print('The best paramter for n_estimator is:', best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest model is developing, it may take 10 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=180, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Develop a random forest model with 'n_estimators' = best_estimators\n",
    "\n",
    "print('random forest model is developing, it may take 10 minutes')\n",
    "best_estimators = 180\n",
    "rfr_best = RandomForestRegressor(n_jobs=-1,n_estimators=best_estimators) \n",
    "rfr_best.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rmodelforest.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save the model in local disk\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rfr_best, 'rmodelforest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest model is running\n",
      "Training AUC:  1.0\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the training AUC performance of the model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rfr_load = joblib.load('rforest.pkl')\n",
    "print('random forest model is running')\n",
    "train_y_pred = rfr_load.predict(train_x)\n",
    "auc_train = roc_auc_score(train_y, train_y_pred)\n",
    "print('Training AUC: ', auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC:  0.9486901912291219\n"
     ]
    }
   ],
   "source": [
    "### Make prediction on test data\n",
    "test_y_pred = rfr_load.predict(test_x)\n",
    "auc_test = roc_auc_score(test_y, test_y_pred)\n",
    "print('Testing AUC: ', auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1:</h1><p>\n",
    "Optimize the model: There are areas for improvement for the base model presented. Modify the code to improve the accuracy of the model. Look for opportunities to improve  performance including data cleaning/preparation, model selection, train/test split, and hyper-parameter tuning. The model performance will be measured by AUC against the holdout test set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Replacement Model</b><br>\n",
    "The replacement model also uses the random forest model. The data underwent additional cleaning. First the categorical data was checked for alternate references to the same value. The conversion from dollars to floats was also improved.<br><br>\n",
    "To check the amount of data and the and parameters where not excessive random values for both were taken and the AUC was measured. A parabaloid was then fit to the values to find the opitimum values for each parameter. As there where few values for this simulation a best fit parabaloid smoothed out any irregularities. This code was written with the ability to combine the results of several simulations.<br><br>\n",
    "The given model also looks like wasn't run enough times with subsets of data to tune hyper parameters. Even replicating the model and running it gave a dramatic improvement in its original AUC score of 0.937. As seen in the data taken for the paraballoid an accuracy of 0.95 (no idea why I didn't keep those values) was acheived an improvement of accuracy of 0.013. As getting this data was parallelizable refining hyperparamaters would also be parallelizable. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(samples=0):\n",
    "    raw_train = pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_train.csv')\n",
    "    raw_test = pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_test.csv')\n",
    "    if samples > 0 and samples < len(raw_train):\n",
    "        raw_train.sample(samples)\n",
    "    raw_train_drop = raw_train.dropna(axis=0, subset=['y'])\n",
    "    list_all = [raw_train_drop,raw_test]\n",
    "    len_train = len(raw_train_drop)\n",
    "    len_test = len(raw_test)\n",
    "    raw = pd.concat(list_all, ignore_index=True, sort=False)\n",
    "    del(raw_train,raw_test,raw_train_drop)\n",
    "    return [raw, len_train, len_test]\n",
    "\n",
    "def get_null(raw):\n",
    "    Null_list = raw.isnull().sum().sort_values(ascending=False)/float(raw.shape[0])*100\n",
    "    print('the pencentage of NUll value in each features are:\\r\\n', Null_list[:10])\n",
    "\n",
    "def remove_features(features, raw):\n",
    "    for feature in features:\n",
    "        del raw[feature]\n",
    "    return raw\n",
    "\n",
    "def remove_dollar(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('$').replace(',','').replace('(','').replace(')',''))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_dollars(features, raw):\n",
    "    for feature in features:\n",
    "        raw[feature] = raw[feature].apply(remove_dollar)\n",
    "    return raw\n",
    "\n",
    "def per_float(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('%'))/100\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_percents(features, raw):\n",
    "    for feature in features:\n",
    "        raw[feature] = raw[feature].apply(per_float)\n",
    "    return raw\n",
    "\n",
    "def set_target_varible(raw):\n",
    "    return raw['y']\n",
    "\n",
    "def set_input_variable_list(raw):\n",
    "    del raw['y']\n",
    "    return raw\n",
    "\n",
    "def get_object_columns(raw):\n",
    "    return raw.dtypes[raw.dtypes == 'object'].index\n",
    "\n",
    "def get_numerical_columns(raw):\n",
    "    return raw.dtypes[raw.dtypes == 'float64'].index\n",
    "\n",
    "def fill_null(raw,value=10**20):\n",
    "    labels=get_numerical_columns(raw)\n",
    "    for label in labels:\n",
    "        raw[label].fillna(value,inplace=True)\n",
    "    return raw    \n",
    "\n",
    "def column_replace(raw, column, given_values, replacement_values):\n",
    "    raw[column].replace(given_values, replacement_values, inplace=True)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weekday_names=['thur', 'wed', 'thurday', 'wednesday', 'fri', 'friday',\\\n",
    "                 'tuesday', 'monday']\n",
    "weekday_names=['Thursday', 'Wednesday', 'Thursday', 'Wednesday', 'Friday',\\\n",
    "                 'Friday', 'Tuesday', 'Monday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_month_names=['Jun', 'Aug', 'July', 'May', 'sept.', 'Oct', 'Apr', 'Feb',\\\n",
    "               'Nov', 'Mar', 'Dev', 'January']\n",
    "month_names=['June', 'August', 'July', 'May', 'September', 'October',\\\n",
    "               'April', 'February', 'November', 'March', 'December',\\\n",
    "               'January']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_location_names = ['asia', 'euorpe', 'america']\n",
    "location_names = ['Asia', 'Europe', 'America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_car_names = ['volkswagon', 'bmw', 'ford', 'chrystler', 'tesla',\\\n",
    "                     'mercades', 'chevrolet', 'nissan']\n",
    "car_names = ['Volkswagen', 'BMW', 'Ford', 'Chrysler', 'Tesla',\\\n",
    "                   'Mercedes', 'Chevrolet', 'Nissan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gets_estimators_number():\n",
    "    sample_size = []\n",
    "    estimators_number = []\n",
    "    AUC=[]\n",
    "    for n in tqdm_notebook(range(5)):\n",
    "        #getting subset of the data\n",
    "        samples = np.random.randint(10000,4040000)\n",
    "        raw_data = loadData(samples)\n",
    "        sub_raw = raw_rata[0]\n",
    "        len_train = raw_data[1]\n",
    "        len_test = raw_data[2]\n",
    "        sub_raw = convert_dollars(['x12'], sub_raw)\n",
    "        sub_raw = convert_percents(['x79'], sub_raw)\n",
    "        sub_raw = column_replace(sub_raw, 'x1', old_weekday_names, weekday_names)\n",
    "        sub_raw = column_replace(sub_raw, 'x54', old_month_names, month_names)\n",
    "        sub_raw = column_replace(sub_raw, 'x82', old_location_names, location_names)\n",
    "        sub_raw = column_replace(sub_raw, 'x84', old_location_names, location_names)\n",
    "        sub_raw_y = sub_raw['y']\n",
    "        del sub_raw['y']\n",
    "        cat_cols = get_object_columns(sub_raw)\n",
    "        num_cols = get_numerical_columns(sub_raw)\n",
    "        sub_raw = fill_null(sub_raw)\n",
    "        LBL = preprocessing.LabelEncoder()\n",
    "        dict_list = []\n",
    "        for i in cat_cols:\n",
    "            sub_raw[i] = LBL.fit_transform(sub_raw[i].fillna('0'))\n",
    "            j = dict(zip(np.arange(len(LBL.classes_)),LBL.classes_))\n",
    "            k = {i:j}\n",
    "            dict_list.append(k)\n",
    "        x = sub_raw[:len_train]\n",
    "        y = sub_raw_y[:len_train]\n",
    "        train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "        holdout_x = sub_raw[len_train:]\n",
    "        holdout_y = sub_raw_y[len_train:]\n",
    "        testN = 180+np.random.randint(0,120)\n",
    "        param_grid = {'n_estimators':[testN]}\n",
    "        CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "        CV_rfr.fit(train_x, train_y)\n",
    "        best_estimators = list(CV_rfr.best_params_.values())[0]\n",
    "        rfr_best = RandomForestRegressor(n_jobs=-1,n_estimators=best_estimators) \n",
    "        rfr_best.fit(train_x, train_y)\n",
    "        joblib.dump(rfr_best, 'rforest.pkl')\n",
    "        rfr_load = joblib.load('rforest.pkl')\n",
    "        train_y_pred = rfr_load.predict(train_x)\n",
    "        auc_train = roc_auc_score(train_y, train_y_pred)\n",
    "        test_y_pred = rfr_load.predict(test_x)\n",
    "        auc_test = roc_auc_score(test_y, test_y_pred)\n",
    "        sample_size.append(samples)\n",
    "        estimators_umber.append(testN)\n",
    "        AUC.append(auc_test)\n",
    "    print(sample_size)\n",
    "    print(estimators_number)\n",
    "    print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7c3b48f3984f2fad649c99d59bb77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3861179, 3594780, 1934837, 680574, 142256]\n",
      "[213, 218, 229, 232, 257]\n",
      "[0.9467715328563835, 0.9473127409008374, 0.9489243613842255, 0.9487746486943772, 0.9489413140652975]\n"
     ]
    }
   ],
   "source": [
    "get_estimators_number()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There is a missing cell here that solved for the paraballoid parameters using matrices.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.46690444618892"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Best n_estimators')\n",
    "2*C[0,0]*C[4,0]/(C[5,0]**2-4*C[0,0]*C[3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3088578.549800607"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Best amount of data')\n",
    "(2*C[3,0]*C[1,0]-C[4,0]*C[5,0])/(C[5,0]**2-4*C[0,0]*C[3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pencentage of NUll value in each features are:\r\n",
      " y      20.000\n",
      "x13     0.034\n",
      "x55     0.034\n",
      "x42     0.034\n",
      "x18     0.032\n",
      "x62     0.030\n",
      "x99     0.030\n",
      "x24     0.030\n",
      "x96     0.028\n",
      "x63     0.028\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data(3088579)\n",
    "raw = raw_data[0]\n",
    "len_train = raw_data[1]\n",
    "len_test = raw_data[2]\n",
    "get_null(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = convert_dollars(['x12'], raw)\n",
    "raw = convert_percents(['x79'], raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = column_replace(raw, 'x1', old_weekday_names, weekday_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = column_replace(raw, 'x54', old_month_names, month_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = column_replace(raw, 'x82', old_location_names, location_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = column_replace(raw, 'x84', old_car_names, car_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y = raw['y']\n",
    "del raw['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = get_object_columns(raw)\n",
    "num_cols = get_numerical_columns(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = fill_null(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label encoding for categorical feature\n",
    "from sklearn import preprocessing\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "dict_list = []\n",
    "for i in cat_cols:\n",
    "    raw[i] = LBL.fit_transform(raw[i].fillna('0'))\n",
    "    j = dict(zip(np.arange(len(LBL.classes_)),LBL.classes_))\n",
    "    k = {i:j}\n",
    "    dict_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = raw[:len_train]\n",
    "y = raw_y[:len_train]\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "holdout_x = raw[len_train:]\n",
    "holdout_y = raw_y[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start the hypermeter grid search for n_estimator, it may take a few minutes\n",
      " hypermeter grid search is over\n",
      "The best paramter for n_estimator is: 255\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [255]\n",
    "}\n",
    "\n",
    "print('start the hypermeter grid search for n_estimator, it may take a few minutes')\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(train_x, train_y)\n",
    "\n",
    "best_estimators = list(CV_rfr.best_params_.values())[0]\n",
    "\n",
    "print(' hypermeter grid search is over')\n",
    "print('The best paramter for n_estimator is:', best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest model is developing, it may take 10 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=255, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Develop a random forest model with 'n_estimators' = best_estimators\n",
    "\n",
    "print('random forest model is developing, it may take 10 minutes')\n",
    "rfr_best = RandomForestRegressor(n_jobs=-1,n_estimators=best_estimators) \n",
    "rfr_best.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rforest.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save the model in local disk\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rfr_best, 'rforest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest model is running\n",
      "Training AUC:  1.0\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the training AUC performance of the model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rfr_load = joblib.load('rforest.pkl')\n",
    "print('random forest model is running')\n",
    "train_y_pred = rfr_load.predict(train_x)\n",
    "auc_train = roc_auc_score(train_y, train_y_pred)\n",
    "print('Training AUC: ', auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC:  0.9494908004890436\n"
     ]
    }
   ],
   "source": [
    "### Make prediction on test data\n",
    "test_y_pred = rfr_load.predict(test_x)\n",
    "auc_test = roc_auc_score(test_y, test_y_pred)\n",
    "print('Testing AUC: ', auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2:</h1><p>\n",
    "Prepare model deployment for production: Update your code to meet common production coding standards and best practices. These include modularization, code quality, proper unit testing, and comments/documentation. This should be completed for all parts of Step 1. The code will be evaluated using tooling that evaluates code coverage and code quality.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Preparing the Model</b><br>\n",
    "Getting ready to apply the model does not require much code. In this model it is assumed that data will be checked prior to getting to the prediciton function. To reduce uncessesary redundancy the data will not be checked again here. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(row):\n",
    "    if not ('clf' in vars() or 'clf' in globals()):\n",
    "        filename = 'rforest.pkl'\n",
    "        clf = joblib.load(filename)\n",
    "    features = row.columns\n",
    "    return clf.predict(row[features])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3:</h1><p>\n",
    "Wrap the model code inside an API: The model must be made callable via API call. The call will pass 1 to N rows of data in JSON format, and expects a N responses each with a predicted class and probability belonging to the predicted class.<br><br>\n",
    "\n",
    "Here is an example curl call to your API:<br><br>\n",
    "\n",
    "curl --request POST --url http://localhost:8080/predict --header 'content-type: application/json' --data '{\"x0\": \"9.521496806\", \"x1\": \"wed\", \"x2\": \"-5.087588682\", \"x3\": \"-17.21471427\", ..., \"x97\": \"2.216918955\", \"x98\": \"-18.64465705\", \"x99\": \"-1.926577376\"}'<br><br>\n",
    "\n",
    "or a batch curl call:<br><br>\n",
    "\n",
    "curl --request POST --url http://localhost:8080/predict --header 'content-type: application/json' --data '[{\"x0\": \"9.521496806\", \"x1\": \"wed\", \"x2\": \"-5.087588682\", \"x3\": \"-17.21471427\", ..., \"x97\": \"2.216918955\", \"x98\": \"-18.64465705\", \"x99\": \"-1.926577376\"},{\"x0\": \"8.415753628\", \"x1\": \"thur\", \"x2\": \"-4.934359322\", \"x3\": \"-6.21844247\", ..., \"x97\": \"6.2714321\", \"x98\": \"-38.057369\", \"x99\": \"-2.76817620\"},...,{\"x0\": \"0.96691828\", \"x1\": \"thursday\", \"x2\": \"-3.86881782\", \"x3\": \"-2.2981827\", ..., \"x97\": \"3.1854471\", \"x98\": \"-33.6058873\", \"x99\": \"-2.02788172\"}]'<br><br>\n",
    "\n",
    "Each of the 10,000 rows in the test dataset will be passed through an API call. The call could be a single batch call w/ all 10,000 rows, or 10,000 individual calls. API should be able to handle either case with minimal impact to performance. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>API</b><br>\n",
    "The API hides its parallel computation in the flask library. Flask is threaded by default. Given the ability Pandas to hold the data in dataframes I would not expect it to be unreasonable for Flask to hold the calls in a stack.<br><br>\n",
    "I'm acutally disappointed I didn't select a model with similar performance but also with the ability to use the GPU for parallel computation. That would have been fun.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('flaskTest.py','w')\n",
    "file.write('import pandas as pd\\r\\n')\n",
    "file.write('import json\\r\\n')\n",
    "file.write('import joblib\\r\\n')\n",
    "file.write('from flask import Flask, jsonify, request\\r\\n\\r\\n')\n",
    "file.write('def prediction(row):\\r\\n')\n",
    "file.write('    filename = \"rforest.pkl\"\\r\\n')\n",
    "file.write('    clf = joblib.load(filename)\\r\\n')\n",
    "file.write('    features=row.columns\\r\\n')\n",
    "file.write('    return clf.predict(row[features]) [0]\\r\\n\\r\\n')\n",
    "file.write('app = Flask(__name__)\\r\\n\\r\\n')\n",
    "file.write('@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\\r\\n')\n",
    "file.write('def calculate():\\r\\n')\n",
    "file.write('    data=request.get_json()[0]\\r\\n')\n",
    "file.write('    df = pd.DataFrame([list(data.values())],columns=list(data.keys()))\\r\\n')\n",
    "file.write('    return str(prediction(df))\\r\\n\\r\\n')\n",
    "file.write('if __name__ == \"__main__\":\\r\\n')\n",
    "file.write('    app.run(host=\"localhost\", port=8080, debug=True)')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Test call</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5450980392156862"
     ]
    }
   ],
   "source": [
    "!curl --request POST --url http://localhost:8080/predict --header 'content-type: application/json' --data '[{\"x0\":\"-18.80182962\", \"x1\":\"3\", \"x2\":\"-6.552565502\", \"x3\":\"9.538783853\", \"x4\":\"1.312835053\", \"x5\":\"-30.91975648\", \"x6\":\"-1.74670099\", \"x7\":\"10.35321926\", \"x8\":\"7.5405277989999995\", \"x9\":\"2.367212697\", \"x10\":\"3.3487117539999995\", \"x11\":\"-54.06989036\", \"x12\":\"567.15\", \"x13\":\"0.065574594\", \"x14\":\"-11.77904987\", \"x15\":\"22.50833577\", \"x16\":\"-13.597729000000001\", \"x17\":\"-14.84269673\", \"x18\":\"-5.87168539\", \"x19\":\"5.247740211\", \"x20\":\"-2.998573817\", \"x21\":\"-1.422839837\", \"x22\":\"2.33840389\", \"x23\":\"-2.555334256\", \"x24\":\"-19.04533265\", \"x25\":\"-74.65301752\", \"x26\":\"60.00202245\", \"x27\":\"-13.48779451\", \"x28\":\"1.9058288469999998\", \"x29\":\"-1.8450353030000002\", \"x30\":\"-18.00345303\", \"x31\":\"-0.335476461\", \"x32\":\"4.359108128\", \"x33\":\"-11.03844639\", \"x34\":\"3.4179060139999997\", \"x35\":\"3.911574551\", \"x36\":\"6.626978633999999\", \"x37\":\"157.34343859999998\", \"x38\":\"-2.545060753\", \"x39\":\"0.101563534\", \"x40\":\"39.40455103\", \"x41\":\"13.55925085\", \"x42\":\"-10.72240312\", \"x43\":\"70.4481205\", \"x44\":\"-10.37225358\", \"x45\":\"9.693022091\", \"x46\":\"-23.60131422\", \"x47\":\"-1.880379031\", \"x48\":\"-5.050058431\", \"x49\":\"-16.3366905\", \"x50\":\"-148.93878130000002\", \"x51\":\"-2.302510723\", \"x52\":\"42.95773684\", \"x53\":\"7.024470695\", \"x54\":\"7\", \"x55\":\"1.1978967409999999\", \"x56\":\"-1.020319325\", \"x57\":\"40.85278578\", \"x58\":\"-1.07447457\", \"x59\":\"-4.671086305\", \"x60\":\"0.917255133\", \"x61\":\"42.22603471\", \"x62\":\"69.41205636\", \"x63\":\"-0.472638781\", \"x64\":\"-0.19482051\", \"x65\":\"38.17640107\", \"x66\":\"-1.7928915909999998\", \"x67\":\"1.886820608\", \"x68\":\"-4.182808393999999\", \"x69\":\"-1.6729802569999999\", \"x70\":\"-0.25559694\", \"x71\":\"11.94328284\", \"x72\":\"-2.3974483280000003\", \"x73\":\"12.63558802\", \"x74\":\"2.2493899219999998\", \"x75\":\"-0.315405289\", \"x76\":\"0.0912948\", \"x77\":\"-8.269190527000001\", \"x78\":\"1.882057053\", \"x79\":\"0.0001\", \"x80\":\"-39.23067599\", \"x81\":\"9.004164193\", \"x82\":\"2\", \"x83\":\"0.920773905\", \"x84\":\"9\", \"x85\":\"-0.41756686299999995\", \"x86\":\"-2.178081012\", \"x87\":\"1.5203101069999998\", \"x88\":\"8.832329334\", \"x89\":\"2.541372381\", \"x90\":\"2.346654626\", \"x91\":\"1.123001753\", \"x92\":\"0.100648931\", \"x93\":\"-2.289469986\", \"x94\":\"-31.6471515\", \"x95\":\"33.02701707\", \"x96\":\"0.18542210399999998\", \"x97\":\"6.273963103\", \"x98\":\"13.97970419\", \"x99\":\"-6.463641615\"}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4:</h1><p>\n",
    "Wrap your API in a Docker image: Create a Dockerfile that builds your API into an image. Write a shell script titled run_api.sh that either runs your image using traditional docker run commands or orchestrates your deployment using Compose, Swarm or Kubernetes (include relevant *.yml config files).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p># Dockfile<br>\n",
    "FROM python:latest<br>\n",
    "WORKDIR /home/ubuntu/docker<br>\n",
    "COPY flaskTest.py .<br>\n",
    "CMD [\"hello.py\", \"-flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/bin/bash<br>\n",
    "docker run -ti --name flaskTest pulkit/flaskTest:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5:</h1><p>\n",
    "Optimize your deployment for enterprise production and scalability: Identify opportunities to optimize your deployment for scalability. Consider how your API might handle a large number of calls (thousands per minute). What additional steps/tech could you add to your deployment in order to make it scalable for enterprise level production. You can incorporate any relevant code (optional), or you can describe your steps in the write-up as part of Step 6.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 6:</h1><p>\n",
    "Submit your work: Please submit all of your code, including relevant python files for the API, data prep, model build, Dockerfile (if relevant, orchestration config files), startup shell script, and a brief write-up documenting justification for your end-to-end process in PDF format. Recommend to tar or zip all files into a single archive for submission.<br><br> \n",
    "Please do not submit the original data back to us. Your work will be scored on model performance - measured by AUC - on the data hold out, API performance and scalability, code quality and coverage, and creativity points based on a review of Step 5.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix A: Data Exploration</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looking at raw data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_train.csv')\n",
    "raw_test=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finding the columns that are not numerical</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.loc[:, raw_train.dtypes=='object'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train=raw_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finding the columns that are ints</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.loc[:, raw_train.dtypes=='int'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finding the columns that are float64</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.loc[:, raw_train.dtypes=='float64'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>94 + 1 + 6 adds up to all of the columns<br><br>\n",
    "Inspecting the object columns</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace week names with integers\n",
    "oldWeekdayNames=['thur', 'wed', 'thurday', 'wednesday', 'fri', 'friday',\\\n",
    "                 'tuesday', 'monday']\n",
    "weekdayIntegers=[4, 3, 4, 3, 5, 5, 2, 1]\n",
    "raw_train['x1'].replace(oldWeekdayNames, weekdayIntegers, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x54'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace month names with integers\n",
    "oldMonthsNames=['Jun', 'Aug', 'July', 'May', 'sept.', 'Oct', 'Apr', 'Feb',\\\n",
    "               'Nov', 'Mar', 'Dev', 'January']\n",
    "monthIntegers=[5, 7, 6, 4, 8, 9, 3, 1, 10, 2, 11, 0]\n",
    "raw_train['x54'].replace(oldMonthsNames, monthIntegers, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x54'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x12'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x12']=raw_train['x12'].str.replace('$','')\n",
    "raw_train['x12']=raw_train['x12'].str.replace('(','')\n",
    "raw_train['x12']=raw_train['x12'].str.replace(')','')\n",
    "raw_train['x12']=raw_train['x12'].str.replace(',','')\n",
    "raw_train['x12']=pd.to_numeric(raw_train['x12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x12'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x79'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x79']=raw_train['x79'].str.replace('%','')\n",
    "raw_train['x79']=pd.to_numeric(raw_train['x79'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x79'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Check for duplicate columns</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,100):\n",
    "    for m in range(n+1,100):\n",
    "        if list(raw_train['x'+str(n)])==list(raw_train['x'+str(m)]):\n",
    "            print('Duplicate columns: x'+str(n)+' '+'x'+str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x84'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing car names\n",
    "mispelledCarNames = ['volkswagon', 'bmw', 'ford', 'chrystler', 'tesla', \\\n",
    "'mercades', 'chevrolet', 'nissan']\n",
    "correctCarNames = ['Volkswagen', 'BMW', 'Ford', 'Chrysler', 'Tesla', \\\n",
    "'Mercedes', 'Chevrolet', 'Nissan']\n",
    "raw_train['x84'].replace(mispelledCarNames, correctCarNames,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x84'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x82'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing location names\n",
    "mispelledLocationNames = ['asia', 'euorpe', 'america']\n",
    "correctLocationNames = ['Asia', 'Europe', 'America']\n",
    "raw_train['x82'].replace(mispelledLocationNames, correctLocationNames,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['x82'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.loc[:, raw_train.dtypes=='object'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix B: Given Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_train.csv')\n",
    "raw_test=pd.read_csv('/mnt/c/users/我的电脑/desktop/JobApplications/StateFarmDataScience/Exercise2/exercise_06_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop NULL values in column x1\n",
    "raw_train_drop = raw_train.dropna(axis=0,subset=['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine train and test data\n",
    "list_all=[raw_train_drop,raw_test]\n",
    "raw = pd.concat(list_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(raw_train_drop)\n",
    "len_test =len(raw_test)\n",
    "print('The size of effective training and test dataset is', len_train, len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(raw_train,raw_test,raw_train_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get percentage of NULL values for each feature\n",
    "Null_list = raw.isnull().sum().sort_values(ascending=False)/float(raw.shape[0])*100\n",
    "print('the pencentage of NUll value in each features are:', Null_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove features not used for modeling\n",
    "del raw['x2']\n",
    "del raw['x3']\n",
    "del raw['x19']\n",
    "\n",
    "#These feature need Nature Language Processing before using, thus increasing the complexity of current model\n",
    "del raw['x10']\n",
    "del raw['x16']\n",
    "del raw['x18']\n",
    "\n",
    "#Remove redundant feature\n",
    "del raw['x8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove $ from dollar amount features\n",
    "def remove_dollar(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('$').replace(',',''))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['x4'] = raw['x4'].apply(remove_dollar)\n",
    "raw['x5'] = raw['x5'].apply(remove_dollar)\n",
    "raw['x6'] = raw['x6'].apply(remove_dollar)\n",
    "raw['x12'] = raw['x12'].apply(remove_dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the variable format from percentage to float\n",
    "def per_float(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        return float(x.strip('%'))/100\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['x30'] = raw['x30'].apply(per_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features to be used in modeling\n",
    "raw['x33'] = raw['x5']/raw['x4']\n",
    "raw['x34'] = raw['x6']/raw['x5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time from string format to float (Number of years since 1900-01-01)\n",
    "def toYears(x):\n",
    "    try:\n",
    "        x = datetime.datetime.strptime(x, \"%b-%Y\")\n",
    "        x = x-datetime.datetime(1900,1,1)\n",
    "        return x.days/365.0\n",
    "    except:\n",
    "        try:\n",
    "            x = datetime.datetime.strptime(x, \"%b-%y\")\n",
    "            if (x - datetime.datetime(2017,12,31)).days> 0:\n",
    "                x = x-datetime.datetime(2000,1,2)\n",
    "                return x.days/365.0\n",
    "            x = x-datetime.datetime(1900,1,1)\n",
    "            return x.days/365.0        \n",
    "        except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['x15'] = raw['x15'].apply(toYears)\n",
    "raw['x23'] = raw['x23'].apply(toYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time difference between issue date and the date opened\n",
    "raw['x35'] = raw['x15']  - raw['x23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set target variable and remove it from input variable list\n",
    "raw_y = raw['y']\n",
    "del raw['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split input variables into numerical features and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = raw.dtypes[raw.dtypes == 'object'].index\n",
    "num_cols = raw.dtypes[raw.dtypes == 'float64'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make statistics analysis on target variables, numerical features and categorical features\n",
    "### This cell is very slow to run analysis. Don't run unless you're interested in viewing individual features'\n",
    "fig= plt.figure(figsize=(5,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "y.plot(kind= 'hist',axes =ax1)\n",
    "plt.title('Histgram for interest rate')\n",
    "plt.show()\n",
    "\n",
    "for i in cat_cols:\n",
    "    fig= plt.figure(figsize=(15,5))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    raw[i].value_counts().plot(kind= 'bar',axes =ax1)\n",
    "    plt.title('Histgram for feature: %s' %(i))\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    raw[i].value_counts(normalize = 'True').plot(kind= 'bar', axes =ax2)\n",
    "    plt.title('Histgram for feature: %s (in percentate)' %(i))\n",
    "    plt.show()\n",
    "\n",
    "for i in num_cols:\n",
    "    a = raw[i]\n",
    "    b = a[abs(a - a.mean()) <=3*a.std()]\n",
    "    fig= plt.figure(figsize=(8,5))\n",
    "    b.plot(kind= 'hist',bins = 10)\n",
    "    plt.title('Histgram of %s' %(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy of raw input, will be used later as input variables in the linear regression model \n",
    "raw_bp_linear = copy.deepcopy(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for tree model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace the Null value with very large number (10**20), let tree model to interpret by itself\n",
    "for i in num_cols:\n",
    "    raw[i].fillna(10**20,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label encoding for categorical feature\n",
    "from sklearn import preprocessing\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "dict_list = []\n",
    "for i in cat_cols:\n",
    "    raw[i] = LBL.fit_transform(raw[i].fillna('0'))\n",
    "    j = dict(zip(np.arange(len(LBL.classes_)),LBL.classes_))\n",
    "    k = {i:j}\n",
    "    dict_list.append(k)\n",
    "\n",
    "# print(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into training and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = raw[:len_train]\n",
    "y = raw_y[:len_train]\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "holdout_x = raw[len_train:]\n",
    "holdout_y = raw_y[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build random forest model\n",
    "\n",
    "from sklearn import metrics\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyper-parameter search: n_estimators\n",
    "start to build random forest model\"\n",
    "To reduce the code running time, the process of hypermeter grid search for (n_estimator)\n",
    "which may take one hour or so. Here we only use the final search result\n",
    "If interested in checking the search process, run the following cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [60,120,180]\n",
    "}\n",
    "\n",
    "print('start the hypermeter grid search for n_estimator, it may take a few minutes')\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(train_x, train_y)\n",
    "\n",
    "best_estimators = CV_rfr.best_params_.values()[0]\n",
    "\n",
    "print(' hypermeter grid search is over')\n",
    "print('The best paramter for n_estimator is:', best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Develop a random forest model with 'n_estimators' = best_estimators\n",
    "\n",
    "print('random forest model is developing, it may take 10 minutes')\n",
    "best_estimators = 180\n",
    "rfr_best = RandomForestRegressor(n_jobs=-1,n_estimators=best_estimators) \n",
    "rfr_best.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model in local disk\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rfr_best, 'rforest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the training AUC performance of the model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rfr_load = joblib.load('rforest.pkl')\n",
    "print('random forest model is running')\n",
    "train_y_pred = rfr_load.predict(train_x)\n",
    "auc_train = roc_auc_score(train_y, train_y_pred)\n",
    "print('Training AUC: ', auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make prediction on test data\n",
    "test_y_pred = rfr_load.predict(test_x)\n",
    "auc_test = roc_auc_score(test_y, test_y_pred)\n",
    "print('Testing AUC: ', auc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
